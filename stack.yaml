AWSTemplateFormatVersion: 2010-09-09
Transform: AWS::Serverless-2016-10-31
Resources:
  UploadBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: "input.image.bucket"
      AccessControl: BucketOwnerFullControl
      NotificationConfiguration:
        TopicConfigurations:
          - Topic: !Ref SNSTopic
            Event: "s3:ObjectCreated:*"
    DependsOn:
      - SNSTopicPolicy
      - SNSTopic

  TargetBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: "target.image.bucket"

  SNSTopic:
    Type: AWS::SNS::Topic
    Properties:
      Subscription:
        - Endpoint:
            Fn::GetAtt:
              - "SQSLambda"
              - "Arn"
          Protocol: "sqs"
        - Endpoint:
            Fn::GetAtt:
              - "SQSRDS"
              - "Arn"
          Protocol: "sqs"
      TopicName: "NewImageUpload"

  SNSTopicPolicy:
    Type: "AWS::SNS::TopicPolicy"
    Properties:
      PolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service: s3.amazonaws.com
            Action: "sns:Publish"
            Resource: !Ref SNSTopic
            Condition:
              ArnEquals:
                aws:SourceArn: !Sub arn:${AWS::Partition}:s3:::input.image.bucket
      Topics:
        - !Ref SNSTopic

  SQSLambda:
    Type: AWS::SQS::Queue
    Properties:
      QueueName: "SQSLambdaPool"
      VisibilityTimeout: 200
      RedrivePolicy:
        deadLetterTargetArn:
          Fn::GetAtt:
            - "SQSLambdaDeadLetterQueue"
            - "Arn"
        maxReceiveCount: 2

  SQSLambdaDeadLetterQueue:
    Type: AWS::SQS::Queue
    Properties:
      QueueName: "SQSLambdaDeadLetterQueue"

  SQSRDS:
    Type: AWS::SQS::Queue
    Properties:
      QueueName: "SQSRDSPool"
      VisibilityTimeout: 200
      RedrivePolicy:
        deadLetterTargetArn:
          Fn::GetAtt:
            - "SQSRDSDeadLetterQueue"
            - "Arn"
        maxReceiveCount: 2

  SQSRDSDeadLetterQueue:
    Type: AWS::SQS::Queue
    Properties:
      QueueName: "SQSRDSDeadLetterQueue"

  SQSQueuePolicy:
    Type: AWS::SQS::QueuePolicy
    Properties:
      Queues:
        - !Ref SQSLambda
        - !Ref SQSRDS
      PolicyDocument:
        Id: AllowIncomingAccess
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - "sns.amazonaws.com"
            Action:
              - sqs:SendMessage
            Resource:
              Fn::GetAtt:
                - SQSLambda
                - Arn
            Condition:
              ArnEquals:
                aws:SourceArn:
                  Ref: SNSTopic
          - Effect: Allow
            Principal:
              Service:
                - "sns.amazonaws.com"
            Action:
              - sqs:SendMessage
            Resource:
              Fn::GetAtt:
                - SQSRDS
                - Arn
            Condition:
              ArnEquals:
                aws:SourceArn:
                  Ref: SNSTopic
    DependsOn:
      - SQSLambda
      - SQSRDS
      - SNSTopic

  LambdaS3:
    Type: "AWS::Lambda::Function"
    DependsOn:
      - LambdaRole
    Properties:
      FunctionName: "Lambda_S3_Copy_New_File"
      Handler: index.handler
      Runtime: python3.7
      MemorySize: 128
      Timeout: 120
      TracingConfig:
        Mode: Active
      Role:
        Fn::GetAtt:
          - LambdaRole
          - Arn
      Code:
        ZipFile: |
          import json
          import boto3
          import urllib.parse

          print('Moving File')

          s3 = boto3.resource('s3')

          def handler(event, context):
              body = json.loads(event['Records'][0]['body'])
              message = json.loads(body['Message'])
              input_bucket = message['Records'][0]['s3']['bucket']['name']
              target_bucket = 'target.image.bucket'
              filename = urllib.parse.unquote_plus(message['Records'][0]['s3']['object']['key'], encoding='utf-8')
              try:
                copy_source = {
                    'Bucket': input_bucket,
                    'Key': filename
                }
                
                target_bucket = s3.Bucket(target_bucket)
                target_bucket.copy(copy_source, filename)
              except Exception as e:
                print(f'Failed to move file { filename } from { input_bucket } to { target_bucket }')
                print(e)
                raise e

  LambdaRDS:
    Type: "AWS::Lambda::Function"
    DependsOn:
      - LambdaRole
      - PyMySQLLayer
    Properties:
      FunctionName: "Lambda_RDS_Add_New_File"
      Handler: index.handler
      Runtime: python3.7
      MemorySize: 128
      Timeout: 120
      TracingConfig:
        Mode: Active
      Role:
        Fn::GetAtt:
          - LambdaRole
          - Arn
      Layers:
        - !Ref PyMySQLLayer
      VpcConfig:
        SecurityGroupIds:
          - !Ref SecurityGroup
        SubnetIds:
          - !Ref Subnet
      Code:
        ZipFile: |
          import json
          import urllib.parse
          import pymysql
          import sys

          print('Copying File')

          rds_host = ''
          name = 'admin'
          password = 'dbinstancetest2022'
          db_name = 'RDSImageDB'
          port = 3306

          try:
              conn = pymysql.connect(host=rds_host,user=name,
                                    passwd=password,db=db_name,
                                    connect_timeout=5,
                                    cursorclass=pymysql.cursors.DictCursor)
          except pymysql.MySQLError as e:
              print(e)
              sys.exit()


          def handler(event, context):
              body = json.loads(event['Records'][0]['body'])
              message = json.loads(body['Message'])
              input_bucket = message['Records'][0]['s3']['bucket']['name']
              target_bucket = 'target.image.bucket'
              filename = urllib.parse.unquote_plus(message['Records'][0]['s3']['object']['key'], encoding='utf-8')
              
              try:
                with conn.cursor() as cur:
                  cur.execute(f'CREATE TABLE IF NOT EXISTS Images ( ImageID int AUTO_INCREMENT NOT NULL, ImageName varchar(255) NOT NULL default "", SourcePath varchar(255) default "", RetouchedPath varchar(255) default "", PRIMARY KEY (ImageID))')
                  conn.commit()
                  
                  cur.execute(
                    f'SELECT EXISTS (SELECT ImageName FROM Images WHERE ImageName="{ filename }")'
                  )

                  if cur:
                    conn.commit()
                    return f'The file { filename } already exists in the database'
                  conn.commit()
                  
                  cur.execute(
                    f'INSERT INTO Images (ImageName, SourcePath, RetouchedPath) VALUES({ filename }, { input_bucket }, default)'
                  )
                  conn.commit()
            
                return f'Successfully added file { filename } to the database'
              except Exception as e:
                print(f'Failed to add file { filename } to the database')
                print(e)


  PyMySQLLayer:
    Type: AWS::Lambda::LayerVersion
    Properties:
      CompatibleRuntimes:
        - python3.6
        - python3.7
      Content:
        S3Bucket: python.layer.bucket
        S3Key: pymysql_layer.zip
      Description: PyMySQL Layer
      LayerName: PyMySQL_Layer
      LicenseInfo: MIT

  LambdaRole:
    Type: "AWS::IAM::Role"
    DependsOn:
      - SQSLambda
      - SQSRDS
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - sts:AssumeRole
      Policies:
        - PolicyName: "lambda_allow_access"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action: ec2:*
                Resource: "*"
              - Effect: Allow
                Action: eds:*
                Resource: "*"
              - Effect: Allow
                Action: s3:*
                Resource: "*"
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: "arn:aws:logs:*:*:*"
              - Effect: Allow
                Action:
                  - sqs:ReceiveMessage
                  - sqs:DeleteMessage
                  - sqs:GetQueueAttributes
                  - sqs:ChangeMessageVisibility
                Resource:
                  - Fn::GetAtt:
                      - SQSLambda
                      - Arn
                  - Fn::GetAtt:
                      - SQSRDS
                      - Arn
              - Effect: Allow
                Action:
                  - xray:PutTraceSegments
                  - xray:PutTelemetryRecords
                Resource: "*"

  LambdaS3EventListener:
    Type: "AWS::Lambda::EventSourceMapping"
    DependsOn:
      - SQSLambda
      - LambdaS3
    Properties:
      BatchSize: 16
      FunctionResponseTypes:
        - ReportBatchItemFailures
      MaximumBatchingWindowInSeconds: 30
      Enabled: true
      EventSourceArn:
        Fn::GetAtt:
          - SQSLambda
          - Arn
      FunctionName:
        Fn::GetAtt:
          - LambdaS3
          - Arn

  LambdaRDSEventListener:
    Type: "AWS::Lambda::EventSourceMapping"
    DependsOn:
      - SQSRDS
      - LambdaRDS
    Properties:
      BatchSize: 4
      FunctionResponseTypes:
        - ReportBatchItemFailures
      MaximumBatchingWindowInSeconds: 30
      Enabled: true
      EventSourceArn:
        Fn::GetAtt:
          - SQSRDS
          - Arn
      FunctionName:
        Fn::GetAtt:
          - LambdaRDS
          - Arn

  RDSInstance:
    Type: AWS::RDS::DBInstance
    Properties:
      DBName: RDSImageDB
      MasterUsername: admin
      MasterUserPassword: "dbinstancetest2022"
      Engine: MySQL
      DBInstanceClass: db.t2.micro
      PubliclyAccessible: true
      AllocatedStorage: "5"
      VPCSecurityGroups: 
        - !Ref SecurityGroup
      DBSubnetGroupName:
        Ref: DBSubnetGroup

  InternetGateway:
    Type: AWS::EC2::InternetGateway

  VPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: 10.0.0.0/16
      EnableDnsSupport: "true"
      EnableDnsHostnames: "true"

  AttachGateway:
    Type: AWS::EC2::VPCGatewayAttachment
    Properties:
      VpcId:
        Ref: VPC
      InternetGatewayId:
        Ref: InternetGateway

  Subnet:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId:
        Ref: VPC
      CidrBlock: 10.0.1.0/24
      AvailabilityZone: "eu-central-1a"

  Subnet2:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId:
        Ref: VPC
      CidrBlock: 10.0.2.0/24
      AvailabilityZone: "eu-central-1b"

  SecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Allow http to client host
      VpcId: 
        Ref: VPC
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 0
          ToPort: 65535
          CidrIp: 0.0.0.0/0
      SecurityGroupEgress:
        - IpProtocol: tcp
          FromPort: 0
          ToPort: 65535
          CidrIp: 0.0.0.0/0

  DBSubnetGroup:
    Type: "AWS::RDS::DBSubnetGroup" 
    Properties: 
      DBSubnetGroupDescription: description
      SubnetIds: 
        - !Ref Subnet
        - !Ref Subnet2
        
    
