AWSTemplateFormatVersion: 2010-09-09
Transform: AWS::Serverless-2016-10-31
Resources:
  UploadBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: 'input.image.bucket'
      AccessControl: BucketOwnerFullControl
      NotificationConfiguration:
        TopicConfigurations:
          - Topic: !Ref SNSTopic
            Event: 's3:ObjectCreated:*'
    DependsOn:
      - SNSTopicPolicy
      - SNSTopic

  TargetBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: 'target.image.bucket'

  SNSTopic:
    Type: AWS::SNS::Topic
    Properties:
      Subscription:
        - Endpoint:
            Fn::GetAtt:
              - "SQSLambda"
              - "Arn"
          Protocol: "sqs"
        - Endpoint:
            Fn::GetAtt:
              - "SQSRDS"
              - "Arn"
          Protocol: "sqs"
      TopicName: "NewImageUpload"

  SNSTopicPolicy:
    Type: 'AWS::SNS::TopicPolicy'
    Properties:
      PolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service: s3.amazonaws.com
            Action: 'sns:Publish'
            Resource: !Ref SNSTopic
            Condition:
              ArnEquals:
                aws:SourceArn: !Sub arn:${AWS::Partition}:s3:::input.image.bucket     
      Topics:
        - !Ref SNSTopic

  SQSLambda: 
    Type: AWS::SQS::Queue
    Properties:
      QueueName: 'SQSLambdaPool'
      VisibilityTimeout: 200
      RedrivePolicy: 
        deadLetterTargetArn: 
          Fn::GetAtt: 
            - "SQSLambdaDeadLetterQueue"
            - "Arn"
        maxReceiveCount: 2

  SQSLambdaDeadLetterQueue: 
    Type: AWS::SQS::Queue
    Properties:
      QueueName: 'SQSLambdaDeadLetterQueue'
    
  SQSRDS: 
    Type: AWS::SQS::Queue
    Properties:
      QueueName: 'SQSRDSPool'
      VisibilityTimeout: 200
      RedrivePolicy: 
        deadLetterTargetArn: 
          Fn::GetAtt: 
            - "SQSRDSDeadLetterQueue"
            - "Arn"
        maxReceiveCount: 2

  SQSRDSDeadLetterQueue: 
    Type: AWS::SQS::Queue
    Properties:
      QueueName: 'SQSRDSDeadLetterQueue'

  SQSQueuePolicy:
    Type: AWS::SQS::QueuePolicy
    Properties:
      Queues:
        - !Ref SQSLambda
        - !Ref SQSRDS
      PolicyDocument:
        Id: AllowIncomingAccess
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - "sns.amazonaws.com"
            Action:
              - sqs:SendMessage
            Resource:
              Fn::GetAtt:
                - SQSLambda
                - Arn
            Condition:
              ArnEquals:
                aws:SourceArn:
                  Ref: SNSTopic
          - Effect: Allow
            Principal:
              Service:
                - "sns.amazonaws.com"
            Action:
              - sqs:SendMessage
            Resource:
              Fn::GetAtt:
                - SQSRDS
                - Arn
            Condition:
              ArnEquals:
                aws:SourceArn:
                  Ref: SNSTopic
    DependsOn:
      - SQSLambda
      - SQSRDS
      - SNSTopic

  LambdaS3:
    Type: 'AWS::Lambda::Function'
    DependsOn:
      - LambdaRole
    Properties:
      FunctionName: 'Lambda_S3_Copy_New_File'
      Handler: index.handler
      Runtime: python3.7
      MemorySize: 128
      Timeout: 120
      TracingConfig:
        Mode: Active
      Role:
        Fn::GetAtt:
          - LambdaRole
          - Arn
      Code:
        ZipFile: |
          import json
          import boto3
          import urllib.parse

          print('Moving File')

          s3 = boto3.resource('s3')

          def handler(event, context):
              body = json.loads(event['Records'][0]['body'])
              message = json.loads(body['Message'])
              input_bucket = message['Records'][0]['s3']['bucket']['name']
              target_bucket = 'target.image.bucket'
              filename = urllib.parse.unquote_plus(message['Records'][0]['s3']['object']['key'], encoding='utf-8')
              try:
                copy_source = {
                    'Bucket': input_bucket,
                    'Key': filename
                }
                
                target_bucket = s3.Bucket(target_bucket)
                target_bucket.copy(copy_source, filename)
              except Exception as e:
                print(f'Failed to move file { filename } from { input_bucket } to { target_bucket }')
                print(e)
                raise e

  LambdaRDS:
    Type: 'AWS::Lambda::Function'
    DependsOn:
      - LambdaRole
    Properties:
      FunctionName: 'Lambda_RDS_Add_New_File'
      Handler: index.handler
      Runtime: python3.7
      MemorySize: 128
      Timeout: 120
      TracingConfig:
        Mode: Active
      Role:
        Fn::GetAtt:
          - LambdaRole
          - Arn
      Code: 
        ZipFile: |
          import json
          import urllib.parse
          import rds_config
          import pymysql
          import sys

          print('Copying File')

          rds_host = rds_config.db_endpoint
          name = rds_config.db_username
          password = rds_config.db_password
          db_name = rds_config.db_name
          port = 3306
          INDEX_COUNTER = 0

          try:
              conn = pymysql.connect(host=rds_host,user=name,
                                    passwd=password,db=db_name,
                                    connect_timeout=5,
                                    cursorclass=pymysql.cursors.DictCursor)
          except pymysql.MySQLError as e:
              print(e)
              sys.exit()

          with conn.cursor() as cur:
              INDEX_COUNTER = cur.execute("select count * from Images")

          conn.commit()

          def handler(event, context):
              body = json.loads(event['Records'][0]['body'])
              message = json.loads(body['Message'])
              input_bucket = message['Records'][0]['s3']['bucket']['name']
              filename = urllib.parse.unquote_plus(message['Records'][0]['s3']['object']['key'], encoding='utf-8')
              
              try:
                with conn.cursor() as cur:
                  INDEX_COUNTER += 1 
                  cur.execute(
                    f'insert into Images (ImageID, ImageName, SourcePath, RetouchedPath) values({ INDEX_COUNTER }, { filename }, { input_bucket }, "")'
                  )
                    
                conn.commit()
            
                return f'Successfully added file { filename } to the database'
              except Exception as e:
                print(f'Failed to add file { filename } to the database')
                print(e)


  LambdaRole:
    Type: 'AWS::IAM::Role'
    DependsOn:
      - SQSLambda
      - SQSRDS
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - sts:AssumeRole
      Policies:
        - PolicyName: 'lambda_allow_access'
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action: s3:*
                Resource: '*'
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: "arn:aws:logs:*:*:*"
              - Effect: Allow
                Action:
                  - sqs:ReceiveMessage
                  - sqs:DeleteMessage
                  - sqs:GetQueueAttributes
                  - sqs:ChangeMessageVisibility
                Resource: 
                  - Fn::GetAtt:
                      - SQSLambda
                      - Arn
                  - Fn::GetAtt:
                      - SQSRDS
                      - Arn
              - Effect: Allow
                Action:
                  - xray:PutTraceSegments
                  - xray:PutTelemetryRecords
                Resource: '*'

  LambdaS3EventListener:
    Type: 'AWS::Lambda::EventSourceMapping'
    DependsOn:
      - SQSLambda
      - LambdaS3
    Properties:
      BatchSize: 16
      FunctionResponseTypes: 
        - ReportBatchItemFailures
      MaximumBatchingWindowInSeconds: 30
      Enabled: true
      EventSourceArn: 
        Fn::GetAtt:
          - SQSLambda
          - Arn
      FunctionName: 
        Fn::GetAtt:
          - LambdaS3
          - Arn

  LambdaRDSEventListener:
    Type: 'AWS::Lambda::EventSourceMapping'
    DependsOn:
      - SQSRDS
      - LambdaRDS
    Properties:
      BatchSize: 4
      FunctionResponseTypes: 
        - ReportBatchItemFailures
      MaximumBatchingWindowInSeconds: 30
      Enabled: true
      EventSourceArn: 
        Fn::GetAtt:
          - SQSRDS
          - Arn
      FunctionName: 
        Fn::GetAtt:
          - LambdaRDS
          - Arn

  RDSInstance:
    Type: AWS::RDS::DBInstance
    Properties:
      DBName: RDSImageDB
      MasterUsername: admin
      MasterUserPassword: ''
      Engine: MySQL
      DBInstanceClass: db.t2.micro
      PubliclyAccessible: true
      AllocatedStorage: "5"